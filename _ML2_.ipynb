{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = make_regression(n_samples=1000 , n_features=3 , noise=10 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(len(y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(X, y, weights):\n",
    "    \n",
    "    #  Calculate predictions by multiplying inputs (X) with weights:)\n",
    "    predictions = X.dot(weights)\n",
    "    \n",
    "    #  Calculate the errors (difference between predictions and actual values):)\n",
    "    errors = predictions - y\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE):)\n",
    "    mse = np.mean(errors ** 2)\n",
    "    \n",
    "    #  Divide by 2 to match the gradient calculation format:)\n",
    "    loss = mse / 2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, num_iterations , freq_print):\n",
    "    alpha = 0.001\n",
    "    W = np.random.randn(X.shape[1], 1)\n",
    "    for i in range(num_iterations) :    \n",
    "        yhat = X.dot(W)\n",
    "        error = yhat - y\n",
    "        gradient = X.T.dot(error)\n",
    "        W = W - alpha * gradient / len(X)\n",
    "        if i % freq_print == 0:\n",
    "            print(calculate_loss(X , y , W))\n",
    "        \n",
    "        \n",
    "    return W             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8405.155870757595\n",
      "1229.9527349777063\n",
      "230.37348851443667\n",
      "79.1554192799069\n",
      "54.417115729699184\n",
      "50.08227705044065\n",
      "49.27929951334994\n",
      "49.12422363920703\n",
      "49.093377975094285\n",
      "49.087118741780614\n"
     ]
    }
   ],
   "source": [
    "W = gradient_descent(X_test , y_test , 10000 , 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.60778446],\n",
       "       [98.19392278],\n",
       "       [81.98373928],\n",
       "       [25.92519061]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function give the most efficient weight and I print the loss for this weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_weight(X, y):\n",
    "    X_transpose = X.T\n",
    "    W = np.linalg.inv(X_transpose @ X) @ X_transpose @ y\n",
    "    \n",
    "    \n",
    "    print(calculate_loss(X , y , W))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the output for the test data withe the most efficient weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.08549457182744\n"
     ]
    }
   ],
   "source": [
    "least_weight(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent final loss output is almost equal to the least loss output.......................\n",
    "                                   49.087106110189694  |||  49.08549457182744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
