{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = make_regression(n_samples=1000 , n_features=3 , noise=10 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(X, y, weights):\n",
    "    \n",
    "    #  Calculate predictions by multiplying inputs (X) with weights:)\n",
    "    predictions = X.dot(weights)\n",
    "    \n",
    "    #  Calculate the errors (difference between predictions and actual values):)\n",
    "    errors = predictions - y\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE):)\n",
    "    mse = np.mean(errors ** 2)\n",
    "    \n",
    "    #  Divide by 2 to match the gradient calculation format:)\n",
    "    loss = mse / 2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, num_iterations , freq_print):\n",
    "    y = y.reshape(len(y),1)\n",
    "    alpha = 0.001\n",
    "    X_new = np.c_[np.ones(len(X)), X]\n",
    "    W = np.random.randn(X_new.shape[1], 1)\n",
    "    for i in range(num_iterations) :    \n",
    "        yhat = X_new.dot(W)\n",
    "        error = yhat - y\n",
    "        gradient = X_new.T.dot(error)\n",
    "        W = W - alpha * gradient / len(X)\n",
    "        if i % freq_print == 0:\n",
    "            print(calculate_loss(X_new , y , W))\n",
    "        \n",
    "        \n",
    "    return W             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8305.39264944929\n",
      "1215.6250337228785\n",
      "228.64705145874316\n",
      "78.99658841973812\n",
      "54.413644754457856\n",
      "50.085767410810604\n",
      "49.28061703773948\n",
      "49.12458062286925\n",
      "49.093463951833364\n",
      "49.087138310512636\n"
     ]
    }
   ],
   "source": [
    "W = gradient_descent(X_test , y_test , 10000 , 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.60795158],\n",
       "       [98.19389296],\n",
       "       [81.98373753],\n",
       "       [25.92503577]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function give the most efficient weight and I print the loss for this weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_weight(X, Y):\n",
    "    Y = Y.reshape(len(Y), 1)\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    X_transpose = X.T\n",
    "    W = np.linalg.inv(X_transpose @ X) @ X_transpose @ Y\n",
    "    \n",
    "    \n",
    "    print(calculate_loss(X , Y , W))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the output for the test data withe the most efficient weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.08549457182744\n"
     ]
    }
   ],
   "source": [
    "least_weight(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent final loss output is almost equal to the least loss output.......................\n",
    "                                   49.087106110189694  |||  49.08549457182744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
